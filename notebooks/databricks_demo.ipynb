{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# beyond-vector-search â€” Databricks demo\n",
        "\n",
        "This notebook runs the repo **inside Databricks** (Repos checkout) without any external network calls.\n",
        "\n",
        "It demonstrates:\n",
        "- **Adaptive retrieval routing** (keyword vs vector)\n",
        "- **Offline evaluation loop** that updates router weights\n",
        "- **SQLite telemetry** inspection\n",
        "\n",
        "> Tip: If you want the telemetry DB to persist across cluster restarts, set `DB_PATH` to a DBFS location (example below).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def find_repo_root(start: Path | None = None) -> Path:\n",
        "    \"\"\"Find the repo root by walking up until we find pyproject.toml.\"\"\"\n",
        "    p = (start or Path.cwd()).resolve()\n",
        "    for _ in range(12):\n",
        "        if (p / \"pyproject.toml\").exists():\n",
        "            return p\n",
        "        p = p.parent\n",
        "    raise RuntimeError(\"Could not find repo root (pyproject.toml not found).\")\n",
        "\n",
        "\n",
        "REPO_ROOT = find_repo_root()\n",
        "SRC_DIR = REPO_ROOT / \"src\"\n",
        "\n",
        "# Make the package importable without pip install.\n",
        "if str(SRC_DIR) not in sys.path:\n",
        "    sys.path.insert(0, str(SRC_DIR))\n",
        "\n",
        "# Telemetry backend selection:\n",
        "# - Default (local/dev): SQLite\n",
        "# - Databricks Lakehouse: Delta tables (\"Lakebase\")\n",
        "#\n",
        "# For Delta telemetry, set these env vars in the cluster (or in a cell below):\n",
        "#   BVS_TELEMETRY=delta\n",
        "#   BVS_DELTA_RUNS_TABLE=<catalog>.<schema>.beyond_vector_search_runs\n",
        "#   BVS_DELTA_STATE_TABLE=<catalog>.<schema>.beyond_vector_search_router_state\n",
        "#\n",
        "# If you keep SQLite, you can set BVS_DB_PATH to a DBFS path like:\n",
        "#   /dbfs/tmp/beyond_vector_search.sqlite\n",
        "DB_PATH = os.environ.get(\"BVS_DB_PATH\", str(REPO_ROOT / \"runs\" / \"beyond_vector_search.sqlite\"))\n",
        "\n",
        "print(\"REPO_ROOT:\", REPO_ROOT)\n",
        "print(\"PYTHONPATH[0]:\", sys.path[0])\n",
        "print(\"BVS_TELEMETRY:\", os.environ.get(\"BVS_TELEMETRY\", \"sqlite\"))\n",
        "print(\"SQLite DB_PATH (if used):\", DB_PATH)\n",
        "print(\"Delta runs table:\", os.environ.get(\"BVS_DELTA_RUNS_TABLE\"))\n",
        "print(\"Delta state table:\", os.environ.get(\"BVS_DELTA_STATE_TABLE\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# --- Choose your Databricks catalog/schema for Delta telemetry ---\n",
        "# If you're not using Unity Catalog, you can try \"hive_metastore.default\".\n",
        "CATALOG = os.environ.get(\"BVS_CATALOG\", \"main\")\n",
        "SCHEMA = os.environ.get(\"BVS_SCHEMA\", \"default\")\n",
        "\n",
        "# Enable Delta telemetry (Lakehouse) by default in Databricks.\n",
        "os.environ.setdefault(\"BVS_TELEMETRY\", \"delta\")\n",
        "os.environ.setdefault(\"BVS_DELTA_RUNS_TABLE\", f\"{CATALOG}.{SCHEMA}.beyond_vector_search_runs\")\n",
        "os.environ.setdefault(\"BVS_DELTA_STATE_TABLE\", f\"{CATALOG}.{SCHEMA}.beyond_vector_search_router_state\")\n",
        "\n",
        "print(\"Using telemetry:\", os.environ[\"BVS_TELEMETRY\"])\n",
        "print(\"Runs table:\", os.environ[\"BVS_DELTA_RUNS_TABLE\"])\n",
        "print(\"State table:\", os.environ[\"BVS_DELTA_STATE_TABLE\"])\n",
        "\n",
        "from beyond_vector_search.run import run_once\n",
        "\n",
        "out = run_once(query=\"How to fix INC-10010?\", k=5, db_path=DB_PATH)\n",
        "out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from beyond_vector_search.evaluate import evaluate_all\n",
        "\n",
        "report = evaluate_all(k=5, db_path=DB_PATH)\n",
        "{\n",
        "  \"mean_score\": report[\"mean_score\"],\n",
        "  \"n\": report[\"n\"],\n",
        "  \"router_state\": report[\"router_state\"],\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect the most recent runs from the Delta runs table\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "runs_df = spark.table(os.environ[\"BVS_DELTA_RUNS_TABLE\"]).orderBy(F.col(\"run_id\").desc()).limit(10)\n",
        "display(runs_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect the current router state stored in the Delta state table\n",
        "state_df = spark.table(os.environ[\"BVS_DELTA_STATE_TABLE\"]).orderBy(\"key\")\n",
        "display(state_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "\n",
        "- If you want to **reset** learning, delete the SQLite file at `DB_PATH` (or point `BVS_DB_PATH` to a new file).\n",
        "- The core decision logic lives in `src/beyond_vector_search/router.py`.\n",
        "- The offline loop that updates weights lives in `src/beyond_vector_search/evaluate.py`.\n",
        "- The architecture diagram is `diagrams/architecture.html`.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
