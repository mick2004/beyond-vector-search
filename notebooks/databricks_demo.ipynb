{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# beyond-vector-search â€” Databricks demo\n",
        "\n",
        "This notebook runs the repo **inside Databricks** (Repos checkout) without any external network calls.\n",
        "\n",
        "It demonstrates:\n",
        "- **Adaptive retrieval routing** (keyword vs vector)\n",
        "- **Offline evaluation loop** that updates router weights\n",
        "- **SQLite telemetry** inspection\n",
        "\n",
        "> Tip: If you want the telemetry DB to persist across cluster restarts, set `DB_PATH` to a DBFS location (example below).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def find_repo_root(start: Path | None = None) -> Path:\n",
        "    \"\"\"Find the repo root by walking up until we find pyproject.toml.\"\"\"\n",
        "    p = (start or Path.cwd()).resolve()\n",
        "    for _ in range(12):\n",
        "        if (p / \"pyproject.toml\").exists():\n",
        "            return p\n",
        "        p = p.parent\n",
        "    raise RuntimeError(\"Could not find repo root (pyproject.toml not found).\")\n",
        "\n",
        "\n",
        "REPO_ROOT = find_repo_root()\n",
        "SRC_DIR = REPO_ROOT / \"src\"\n",
        "\n",
        "# Make the package importable without pip install.\n",
        "if str(SRC_DIR) not in sys.path:\n",
        "    sys.path.insert(0, str(SRC_DIR))\n",
        "\n",
        "# Telemetry backend selection:\n",
        "# - Default (local/dev): SQLite\n",
        "# - Databricks: Lakebase Postgres (OLTP)\n",
        "#\n",
        "# For Lakebase telemetry, set these env vars in the cluster (recommended) or in a cell:\n",
        "#   BVS_TELEMETRY=lakebase\n",
        "#   BVS_LAKEBASE_DSN=postgresql://USER:PASSWORD@HOST:5432/DBNAME\n",
        "#   BVS_LAKEBASE_RUNS_TABLE=beyond_vector_search_runs\n",
        "#   BVS_LAKEBASE_STATE_TABLE=beyond_vector_search_router_state\n",
        "#\n",
        "# If you keep SQLite, you can set BVS_DB_PATH to a DBFS path like:\n",
        "#   /dbfs/tmp/beyond_vector_search.sqlite\n",
        "DB_PATH = os.environ.get(\"BVS_DB_PATH\", str(REPO_ROOT / \"runs\" / \"beyond_vector_search.sqlite\"))\n",
        "\n",
        "print(\"REPO_ROOT:\", REPO_ROOT)\n",
        "print(\"PYTHONPATH[0]:\", sys.path[0])\n",
        "print(\"BVS_TELEMETRY:\", os.environ.get(\"BVS_TELEMETRY\", \"sqlite\"))\n",
        "print(\"SQLite DB_PATH (if used):\", DB_PATH)\n",
        "print(\"Lakebase DSN set:\", bool(os.environ.get(\"BVS_LAKEBASE_DSN\")))\n",
        "print(\"Lakebase runs table:\", os.environ.get(\"BVS_LAKEBASE_RUNS_TABLE\"))\n",
        "print(\"Lakebase state table:\", os.environ.get(\"BVS_LAKEBASE_STATE_TABLE\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# --- Lakebase Postgres telemetry (Databricks OLTP) ---\n",
        "# Set a Lakebase Postgres DSN. Recommended: store credentials in Databricks Secrets and build DSN here.\n",
        "# Example DSN format:\n",
        "#   postgresql://USER:PASSWORD@HOST:5432/DBNAME\n",
        "#\n",
        "# You can set this as a cluster env var instead:\n",
        "#   BVS_LAKEBASE_DSN=...\n",
        "#\n",
        "# NOTE: This requires a Postgres driver in the cluster (psycopg or psycopg2).\n",
        "\n",
        "os.environ.setdefault(\"BVS_TELEMETRY\", \"lakebase\")\n",
        "\n",
        "# TODO: replace with your real DSN (prefer secrets):\n",
        "# os.environ[\"BVS_LAKEBASE_DSN\"] = \"postgresql://...\"\n",
        "\n",
        "# Optional: customize table names in the Lakebase database\n",
        "os.environ.setdefault(\"BVS_LAKEBASE_RUNS_TABLE\", \"beyond_vector_search_runs\")\n",
        "os.environ.setdefault(\"BVS_LAKEBASE_STATE_TABLE\", \"beyond_vector_search_router_state\")\n",
        "\n",
        "print(\"Using telemetry:\", os.environ.get(\"BVS_TELEMETRY\"))\n",
        "print(\"Runs table:\", os.environ.get(\"BVS_LAKEBASE_RUNS_TABLE\"))\n",
        "print(\"State table:\", os.environ.get(\"BVS_LAKEBASE_STATE_TABLE\"))\n",
        "print(\"DSN set:\", bool(os.environ.get(\"BVS_LAKEBASE_DSN\")))\n",
        "\n",
        "from beyond_vector_search.run import run_once\n",
        "\n",
        "out = run_once(query=\"How to fix INC-10010?\", k=5, db_path=DB_PATH)\n",
        "out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from beyond_vector_search.evaluate import evaluate_all\n",
        "\n",
        "report = evaluate_all(k=5, db_path=DB_PATH)\n",
        "{\n",
        "  \"mean_score\": report[\"mean_score\"],\n",
        "  \"n\": report[\"n\"],\n",
        "  \"router_state\": report[\"router_state\"],\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect the most recent runs (Lakebase Postgres)\n",
        "import os\n",
        "\n",
        "dsn = os.environ[\"BVS_LAKEBASE_DSN\"]\n",
        "runs_table = os.environ.get(\"BVS_LAKEBASE_RUNS_TABLE\", \"beyond_vector_search_runs\")\n",
        "\n",
        "try:\n",
        "    import psycopg  # type: ignore\n",
        "\n",
        "    conn = psycopg.connect(dsn)\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(f\"SELECT run_id, ts_unix, strategy, score, query FROM {runs_table} ORDER BY run_id DESC LIMIT 10\")\n",
        "    rows = cur.fetchall()\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "    rows\n",
        "except Exception:\n",
        "    import psycopg2  # type: ignore\n",
        "\n",
        "    conn = psycopg2.connect(dsn)\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(f\"SELECT run_id, ts_unix, strategy, score, query FROM {runs_table} ORDER BY run_id DESC LIMIT 10\")\n",
        "    rows = cur.fetchall()\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "    rows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect the current router state (Lakebase Postgres)\n",
        "import os\n",
        "\n",
        "dsn = os.environ[\"BVS_LAKEBASE_DSN\"]\n",
        "state_table = os.environ.get(\"BVS_LAKEBASE_STATE_TABLE\", \"beyond_vector_search_router_state\")\n",
        "\n",
        "try:\n",
        "    import psycopg  # type: ignore\n",
        "\n",
        "    conn = psycopg.connect(dsn)\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(f\"SELECT key, value_json FROM {state_table} ORDER BY key\")\n",
        "    rows = cur.fetchall()\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "    rows\n",
        "except Exception:\n",
        "    import psycopg2  # type: ignore\n",
        "\n",
        "    conn = psycopg2.connect(dsn)\n",
        "    cur = conn.cursor()\n",
        "    cur.execute(f\"SELECT key, value_json FROM {state_table} ORDER BY key\")\n",
        "    rows = cur.fetchall()\n",
        "    cur.close()\n",
        "    conn.close()\n",
        "    rows\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "\n",
        "- If you want to **reset** learning, delete the SQLite file at `DB_PATH` (or point `BVS_DB_PATH` to a new file).\n",
        "- The core decision logic lives in `src/beyond_vector_search/router.py`.\n",
        "- The offline loop that updates weights lives in `src/beyond_vector_search/evaluate.py`.\n",
        "- The architecture diagram is `diagrams/architecture.html`.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
